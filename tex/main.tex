\documentclass[draft]{agujournal2019}
\usepackage{url} %this package should fix any errors with URLs in refs.
\usepackage{lineno}
\usepackage[inline]{trackchanges} %for better track changes. finalnew option will compile document with changes incorporated.
\usepackage{soul}
\linenumbers

% --- My additions
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools} % for coloneqq
\usepackage{todonotes}
\usepackage{overpic}
\usepackage{rnn}
\usepackage[capitalise,noabbrev]{cleveref}

\crefname{appendix}{}{} % gets rid of Appendix Appendix A

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\newcommand{\citep}{\cite}
\newcommand{\citet}{\citeA}

%%%%%%%
% As of 2018 we recommend use of the TrackChanges package to mark revisions.
% The trackchanges package adds five new LaTeX commands:
%
%  \note[editor]{The note}
%  \annote[editor]{Text to annotate}{The note}
%  \add[editor]{Text to add}
%  \remove[editor]{Text to remove}
%  \change[editor]{Text to remove}{Text to add}
%
% complete documentation is here: http://trackchanges.sourceforge.net/
%%%%%%%

\draftfalse
\journalname{Journal of Advances in Modeling Earth Systems (JAMES)}


\begin{document}

\title{Temporal Subsampling Diminishes Small Scales in
    Recurrent Neural Network Emulators of
    Geophysical Turbulence}

%% ------------------------------------------------------------------------ %%
%
%  AUTHORS AND AFFILIATIONS
%
%% ------------------------------------------------------------------------ %%

% Example: \authors{A. B. Author\affil{1}\thanks{Current address, Antartica}, B. C. Author\affil{2,3}, and D. E.
% Author\affil{3,4}\thanks{Also funded by Monsanto.}}

\authors{
Timothy A. Smith\affil{1,2},
Stephen G. Penny\affil{1,3},
Jason A. Platt\affil{4},
Tse-Chun Chen\affil{1,2}
}
\affiliation{1}{Cooperative Institute for Research in Environmental Sciences
    (CIRES) at the University of Colorado Boulder, Boulder, CO, USA
}
\affiliation{2}{Physical Sciences Laboratory (PSL), National Oceanic and
    Atmospheric Administration (NOAA), Boulder, CO, USA
}
\affiliation{3}{Sofar Ocean Technologies, San Francisco, CA, USA}
\affiliation{4}{University of California San Diego (UCSD), La Jolla, CA, USA}

\correspondingauthor{Timothy A. Smith}{tim.smith@noaa.gov}

\begin{keypoints}
    \item Reducing training data temporal resolution by subsampling leads to
        overly dissipative small spatial scales in neural network
        emulators
    \item A quadratic autoregressive architecture is shown to be inadequate at capturing
        small scale turbulence, even when data are not subsampled
    \item Echo state networks perform better, and subsampling bias is mitigated
        but not eliminated by prioritizing kinetic energy spectrum in training
\end{keypoints}

%% ------------------------------------------------------------------------ %%
%
%  ABSTRACT and PLAIN LANGUAGE SUMMARY
%
%% ------------------------------------------------------------------------ %%

\begin{abstract}
    The immense computational cost of traditional numerical weather and climate models has sparked the development of machine learning (ML) based emulators or surrogate
    models. Because ML methods benefit from long records of training data,
    it is common to use datasets that are temporally subsampled relative to the time steps required by numerical integration of differential equation models.
    Here, we investigate how this often overlooked processing step affects
    the quality of an emulator's predictions. We implement two ML architectures from a class of methods called reservoir computing: (1) a form of Nonlinear Vector Autoregression (NVAR), and (2) an Echo State Network (ESN).
    Despite their simplicity, it is well documented that these architectures
    excel at predicting low dimensional chaotic dynamics. We are therefore motivated to test these architectures in an idealized setting of predicting high dimensional geophysical turbulence as represented by Surface Quasi-Geostrophic dynamics.
    In all cases, we see that subsampling the training data consistently leads to a bias at small spatial scales that resembles numerical diffusion.
    Interestingly, the NVAR architecture becomes unstable when the temporal
    resolution is increased, indicating that the polynomial based interactions
    are insufficient at capturing the detailed nonlinearities present in the presence of turbulence.
    The ESN architecture is found to be more robust, suggesting a benefit to the more expensive but more general structure.
    We show that spectral errors can be reduced by including a penalty on the kinetic energy density spectrum during training, although the subsampling related errors persist.
    Future work is warranted to understand how the temporal resolution
    of training data affects other neural network architectures.
\end{abstract}

\section*{Plain Language Summary}

The computer models that govern weather prediction and climate projections
are extremely costly to run, causing practitioners to make unfortunate
tradeoffs between accuracy of the physics and credibility of their statistics.
Recent advances in machine learning have sparked the development of neural
network-based emulators, i.e., low-cost models that can be used as drop-in replacements for the traditional expensive models. Due to the cost of storing large weather and climate datasets, it is common to subsample these fields in time to save disk space and reduce computational expense during training. Here, we show that this pre-processing step hinders the fidelity of the emulator. We offer one method to mitigate the resulting errors, but we suggest that more research is needed to understand and eventually overcome them.

%% ------------------------------------------------------------------------ %%
%
%  TEXT
%
%% ------------------------------------------------------------------------ %%

\input{introduction.tex}
\input{sqg.tex}
\input{methods.tex}
\input{nvar.tex}
\input{esn.tex}
\input{discussion.tex}

\appendix
\input{new_methods.tex}
\input{gom.tex}


\newpage
\section{Open Research}
Code will be made available prior to publication.
%AGU requires an Availability Statement for the underlying data needed to understand, evaluate, and build upon the reported research at the time of peer review and publication.
%
%Authors should include an Availability Statement for the software that has a significant impact on the research. Details and templates are in the Availability Statement section of the Data and Software for Authors Guidance: \url{https://www.agu.org/Publish-with-AGU/Publish/Author-Resources/Data-and-Software-for-Authors#availability}
%
%It is important to cite individual datasets in this section and, and they must be included in your bibliography. Please use the type field in your bibtex file to specify the type of data cited. Some options include Dataset, Software, Collection, ComputationalNotebook. Ex:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\acknowledgments

T.A. Smith and S.G. Penny acknowledge support from NOAA grant NA20OAR4600277.
S.G. Penny and J.A. Platt acknowledge support from the Office of Naval Research
(ONR) grants N00014-19-1-2522 and N00014-20-1-2580. T.-C. Chen is supported by
the NOAA Cooperative Agreement with CIRES, NA17OAR4320101.


%% ------------------------------------------------------------------------ %%
%% References and Citations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% don't specify bibliographystyle
\bibliography{references}
\end{document}
