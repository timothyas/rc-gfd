\section{Discussion}
\label{sec:discussion}

Weather and climate forecasting necessitates the integration of expensive
numerical models to make accurate predictions and projections.
The computational cost of these models often results in tradeoffs, where
practitioners must balance the spatial resolution of their model with other
factors, such as the number of integrated model components or the ensemble
size that can be afforded in the system.
Model emulation or surrogate modeling aims to enable such predictions
by emulating the dynamical system with adequate accuracy at a much lower
computational expense.
In this study, our primary interest was to shed light on the spatial scales
that can be resolved by recurrent neural network emulators in order to better
understand the effective resolution that could be achieved in weather and
climate applications.
We used two relatively simple, single layer autoregressive and recurrent neural
network architectures, mainly because it has been shown that they can
successfully emulate low dimensional chaotic dynamics over multiple Lyapunov
timescales
\citep{pathak_using_2017,vlachas_backpropagation_2020,gauthier_next_2021,platt_systematic_2022}.
We implemented a multi-dimensional parallelization scheme based on the concept
introduced by \citet{pathak_model-free_2018} and similar to that of
\citet{arcomano_machine_2020}
in order to scale up these architectures and test them in high dimensional
systems.
We note that an in-depth discussion of our software implementation using the
task based scheduling system in python, Dask \citep{dask_2016}, will be covered in a
forthcoming paper.

Our main result is that we observe an inherent spectral bias that occurs when
training data are subsampled in time, such that as the temporal resolution is
reduced, the resolution of small scale features in neural
network predictions is diminished.
High wavenumber spectral bias is a phenomenon
that has been studied extensively, especially in the context of training feed forward neural
networks \citep<see>[for a comprehensive review on the topic]{xu_overview_2022}.
The authors show that while
numerical Partial Differential Equation (PDE) solvers typically resolve small spatial scales first
and iteratively refine the larger spatial scales,
spectral biases arise while training neural
networks because the reverse happens: the large scales are uncovered first and
small spatial scales are slowly refined.
Here, we showed a similar bias that arises in NVAR and ESN architectures in relation
to their temporal resolution.
Given the sensitivity to model time step, this phenomenon bears
resemblance to the Courant-Friedrich-Lewy (CFL) condition, which poses an upper
bound on the time step size that can be used in the numerical solution of PDEs.
The CFL condition is therefore a barrier to weather and climate model efficiency.
However, sensitivity to the time step size manifests very
differently in neural networks and numerical PDEs.
While violating the CFL condition with too large of a time step leads to
fundamental issues of numerical instability in numerical PDEs,
here we see that increasing the time step adds a sort of numerical dissipation,
which can actually stabilize an otherwise unstable model architecture
(\cref{subsec:nvar-subsampling}).
We suggest that this occurs because the small scales are ``lost'' within the recurrent and
autoregressive time stepping relations.
Because of this, the models are trained to take on
an interpolated or spatially averaged view of the intermediate dynamical
behavior, which generates a blurred prediction.

This result has important implications for the rapidly developing field of
neural network emulation for weather and climate forecasting because it shows a
potential limit to the effective resolution of an emulator relative to the
original training data.
If an emulator is used as a parameterization scheme for sub-grid-scale dynamics
\citep<e.g.,>[]{perezhogin_generative_2023}, then a high wavenumber spectral
bias will be detrimental to performance.
Additionally, we surmise that such errors will reduce ensemble spread within
data assimilation algorithms, which could limit their usefulness within a
forecasting system \citep<e.g.,>[]{kalnay_ensemble_2006}.
Our findings are pertinent to the field of neural network emulation development
because of the
widespread usage of reanalysis datasets for training.
Currently, most existing neural network emulators in this field use the ERA5
reanalysis dataset \citep{hersbach_era5_2020} for training
\citep<e.g.>[]{lam_graphcast_2022,bi_pangu-weather_2022,pathak_fourcastnet_2022,keisler_forecasting_2022,weyn_sub-seasonal_2021,arcomano_machine_2020}.
Of course, reanalyses like ERA5 are an obvious choice for many reasons:
the datasets are made freely available, they present a multi-decadal view of
weather and climate, and, most importantly, they are constrained to observational
data.
However, we note that reanalysis products are imperfect for at least the following reasons:
they contain jumps in the system state at the start of each DA cycle,
they may contain inconsistencies reflective of changes in observational
coverage,
and they are only made available at large time intervals relative to the time step of the underlying integrated numerical model dynamics, due to the massive size of the data.
Our study only addressed the latter of these issues, and showed that this simple
space-saving step can have a negative impact on data-driven prediction methods.
While we showed that adding spectral error as a weak constraint in the neural
network training can reduce this time step related spectral bias, our results
indicate that the underlying issue persists (\cref{subsec:esn-subsampling}).
Moreover, as long as the data are not subsampled, we showed that ESNs
perform only marginally worse when $<1$~year of data are used, compared to
15~years of training data (\cref{subsec:esn-fixed-steps}).
This result suggests that it may be more effective to design an RNN-based emulator
with a relatively short model trajectory that is not subsampled, rather than a long
trajectory that is subsampled.
In contrast to training the emulator on a reanalysis dataset,
a pure model-based emulator could then be used within a data assimilation system as in
\citet{penny_integrating_2022}
in order to additionally benefit from observational constraints.

Due to the fact that RNNs require long, sequential data streams in order to
learn the governing dynamics, it could be the case that RNNs suffer most
dramatically from temporal subsampling.
This hypothesis could be one reason for why the RNNs used by
\citet{agarwal_comparison_2021} performed worse than other models on data that
were subsampled every 10~days.
Additionally, if RNNs are most dramatically affected by temporal subsampling, then
they could be a suboptimal
architecture choice for model emulators in cases where representing small-scale
dynamics is important but a coarse time step is required.
This requirement is especially true when designing a
parameterization scheme for sub-grid-scale dynamics, where the emulator should ideally run at the same time step as the
``large-scale'' model.
However, given that we can qualitatively observe some degree of spectral error in
a wide variety of neural network architectures that use subsampled data for
training
\citep<e.g.,>[]{bi_pangu-weather_2022,pathak_fourcastnet_2022,keisler_forecasting_2022},
the issue could be more general to other neural network
architectures.
Therefore, we suggest that future work should be directed at understanding the degree to which
temporal resolution affects architectures other than RNNs.
Potential avenues
could include exploring how attention mechanisms
\citep{vaswani_attention_2017,dosovitskiy_image_2021}
handle this phenomenon.
Additionally, in light of our results indicating that wider networks can
mitigate the spectral bias at least to some degree (\cref{subsec:esn-size}), it would be
instructive to understand how successively adding layers to a neural
network affects the spectral bias.
Finally, we note the work of \citet{duncan_generative_2022} who show success in
using adversarial training to mitigate the spectral bias observed in
FourCastNet, and suggest that such techniques deserve additional study to
understand their robustness.

Of course, our neural network implementations are imperfect, and we suggest some
future avenues to improve their predictive capabilities.
The NVAR architecture that we employed is incredibly simple.
While we supposed that the local quadratic feature vector could learn
quantities like derivatives and fluxes necessary to step the model forward in
time, it is apparently not robust enough
given the dramatic sensitivity to time step used.
Future work could explore the possibility of using a larger library of analytic functions to improve the nonlinear expressions in the model,
with the caution that this will lead to very high dimensional feature vectors. Such developments must sufficiently address the ``Catch-22''
described by \citet{zhang_catch-22_2022}, who show that NVAR is inherently
sensitive to the types of nonlinearity chosen.
It is entirely possible, though, that an appropriate set of such basis functions exist for weather and climate emulation.

The ESN architecture that we employed is also relatively straightforward, and
can undoubtedly be improved.
In this work we took a somewhat brute force approach to emulate arbitrarily
high dimensional systems by partitioning the system into subdomains and
deploying parallel ESNs on each group.
However, this process comes with overhead and can still lead to rather large
networks on each group.
Future work could explore dimension reduction techniques involving
proper orthogonal decomposition \citep{jordanou_investigation_2022},
autoencoders \citep{heyder_generalizability_2022},
or approaches involving self-organizing or scale invariant maps
\citep{basterrech_self-organizing_2011}.
Similarly, \citet{whiteaker_reducing_2022}
show success in deriving a controllability matrix for the
ESN, which leads to a reduced network size with minimal reduction in
error.
Finally, a number of studies claim to have developed ESN architectures that
can capture dynamics occurring at many scales
\citep{moon_hierarchical_2021,ma_deepr-esn_2020,gallicchio_design_2018,gallicchio_deep_2017,malik_multilayered_2017},
and these could be
explored for geophysical turbulence emulation as well.
