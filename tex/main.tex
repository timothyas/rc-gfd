\documentclass[draft]{agujournal2019}
\usepackage{url} %this package should fix any errors with URLs in refs.
\usepackage{lineno}
\usepackage[inline]{trackchanges} %for better track changes. finalnew option will compile document with changes incorporated.
\usepackage{soul}
\linenumbers

% --- My additions
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools} % for coloneqq
\usepackage{todonotes}
\usepackage{overpic,pict2e}
\usepackage{rnn}
\usepackage[capitalise,noabbrev]{cleveref}

\crefname{appendix}{}{} % gets rid of Appendix Appendix A

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\newcommand{\citep}{\cite}
\newcommand{\citet}{\citeA}

\newcommand{\noopsort}[2]{#2}

%%%%%%%
% As of 2018 we recommend use of the TrackChanges package to mark revisions.
% The trackchanges package adds five new LaTeX commands:
%
%  \note[editor]{The note}
%  \annote[editor]{Text to annotate}{The note}
%  \add[editor]{Text to add}
%  \remove[editor]{Text to remove}
%  \change[editor]{Text to remove}{Text to add}
%
% complete documentation is here: http://trackchanges.sourceforge.net/
%%%%%%%

\draftfalse
\journalname{Journal of Advances in Modeling Earth Systems (JAMES)}


\begin{document}

\title{Temporal Subsampling Diminishes Small Spatial Scales in
    Recurrent Neural Network Emulators of
    Geophysical Turbulence}

%% ------------------------------------------------------------------------ %%
%
%  AUTHORS AND AFFILIATIONS
%
%% ------------------------------------------------------------------------ %%

% Example: \authors{A. B. Author\affil{1}\thanks{Current address, Antartica}, B. C. Author\affil{2,3}, and D. E.
% Author\affil{3,4}\thanks{Also funded by Monsanto.}}

\authors{
Timothy A. Smith\affil{1,2},
Stephen G. Penny\affil{1,3},
Jason A. Platt\affil{4},
Tse-Chun Chen\affil{1,2}
}
\affiliation{1}{Cooperative Institute for Research in Environmental Sciences
    (CIRES) at the University of Colorado Boulder, Boulder, CO, USA
}
\affiliation{2}{Physical Sciences Laboratory (PSL), National Oceanic and
    Atmospheric Administration (NOAA), Boulder, CO, USA
}
\affiliation{3}{Sofar Ocean Technologies, San Francisco, CA, USA}
\affiliation{4}{University of California San Diego (UCSD), La Jolla, CA, USA}
\affiliation{5}{Pacific Northwest National Laboratory, Richland, WA, USA }

\correspondingauthor{Timothy A. Smith}{tim.smith@noaa.gov}

\begin{keypoints}
    \item Reducing training data temporal resolution by subsampling leads to
        overly dissipative small spatial scales in neural network
        emulators
    \item A quadratic autoregressive architecture is shown to be inadequate at capturing
        small scale turbulence, even when data are not subsampled
    \item Subsampling bias in Echo State Networks is mitigated but not
        eliminated by prioritizing kinetic energy spectrum during training
\end{keypoints}

%% ------------------------------------------------------------------------ %%
%
%  ABSTRACT and PLAIN LANGUAGE SUMMARY
%
%% ------------------------------------------------------------------------ %%

\begin{abstract}
    The immense computational cost of traditional numerical weather and climate
    models has sparked the development of machine learning (ML) based emulators.
    Because ML methods benefit from long records of training data,
    it is common to use datasets that are temporally subsampled relative to the
    time steps required for the numerical integration of differential equations.
    Here, we investigate how this often overlooked processing step affects
    the quality of an emulator's predictions.
    We implement two ML architectures
    from a class of methods called reservoir computing: (1) a form of Nonlinear
    Vector Autoregression (NVAR), and (2) an Echo State Network (ESN).
    Despite their simplicity, it is well documented that these architectures
    excel at predicting low dimensional chaotic dynamics.
    We are therefore
    motivated to test these architectures in an idealized setting of predicting
    high dimensional geophysical turbulence as represented by Surface
    Quasi-Geostrophic dynamics.
    In all cases, subsampling the training data consistently leads
    to an increased bias at small spatial scales that resembles numerical diffusion.
    Interestingly, the NVAR architecture becomes unstable when the temporal
    resolution is increased, indicating that the polynomial based interactions
    are insufficient at capturing the detailed nonlinearities of the turbulent
    flow.
    The ESN architecture is found to be more robust, suggesting a benefit to the
    more expensive but more general structure.
    Spectral errors are reduced by including a penalty on the
    kinetic energy density spectrum during training, although the subsampling
    related errors persist.
    Future work is warranted to understand how the temporal resolution
    of training data affects other ML architectures.
\end{abstract}

\section*{Plain Language Summary}

The computer models that govern weather prediction and climate projections
are extremely costly to run, causing practitioners to make unfortunate
tradeoffs between accuracy of the physics and credibility of their statistics.
Recent advances in machine learning have sparked the development of neural
network-based emulators, i.e., low-cost models that can be used as drop-in
replacements for the traditional expensive models. Due to the cost of storing
large weather and climate datasets, it is common to subsample these fields in
time to save disk space.
This subsampling also reduces the computational expense of training emulators.
Here, we show that this pre-processing step hinders the fidelity of the emulator. We
offer one method to mitigate the resulting errors, but we suggest that more
research is needed to understand and eventually overcome them.

%% ------------------------------------------------------------------------ %%
%
%  TEXT
%
%% ------------------------------------------------------------------------ %%

\input{introduction.tex}
\input{sqg.tex}
\input{methods.tex}
\input{nvar.tex}
\input{esn.tex}
\input{discussion.tex}

\appendix
\input{new_methods.tex}
\input{gom.tex}


\newpage
\section{Open Research}
Code will be made available prior to publication.
%AGU requires an Availability Statement for the underlying data needed to understand, evaluate, and build upon the reported research at the time of peer review and publication.
%
%Authors should include an Availability Statement for the software that has a significant impact on the research. Details and templates are in the Availability Statement section of the Data and Software for Authors Guidance: \url{https://www.agu.org/Publish-with-AGU/Publish/Author-Resources/Data-and-Software-for-Authors#availability}
%
%It is important to cite individual datasets in this section and, and they must be included in your bibliography. Please use the type field in your bibtex file to specify the type of data cited. Some options include Dataset, Software, Collection, ComputationalNotebook. Ex:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\acknowledgments

T.A. Smith and S.G. Penny acknowledge support from NOAA grant NA20OAR4600277.
S.G. Penny and J.A. Platt acknowledge support from the Office of Naval Research
(ONR) grants N00014-19-1-2522 and N00014-20-1-2580.


%% ------------------------------------------------------------------------ %%
%% References and Citations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% don't specify bibliographystyle
\bibliography{references}
\end{document}
