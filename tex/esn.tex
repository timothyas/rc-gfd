\section{Echo State Network Prediction Skill}
\label{sec:esn-results}

In this section we show the prediction skill of the more general ESN architecture outlined in \cref{subsec:rc}.
Here we use similar metrics as in \cref{sec:nvar-results} to evaluate the ESN
skill, except that we show time averaged quantitative metrics because all of the
ESN predictions are stable for the full twelve-hour forecast horizon.
That is, when shown as a single distribution rather than a time series, NRMSE is reported as
\begin{linenomath*}\begin{equation}
    \text{NRMSE} = \sqrt{
            \dfrac{1}{\ntime\nstate}\sum_{n=1}^{\ntime}\sum_{i=1}^{\nstate}\left(
        \dfrac{\hat{v}_i(n) - v_i(n)}{SD}
        \right)^2 } \, ,
    \label{eq:total-nrmse}
\end{equation}\end{linenomath*}
where $\ntime$ consists of the number of timesteps in the trajectory.
In order to characterize spectral error, we show the KE relative error as in
\cref{sec:nvar-results}.
Additionally, we show the NRMSE in terms of the KE density spectrum as follows
\begin{linenomath*}\begin{equation}
    \text{KE\_NRMSE} = \sqrt{
            \dfrac{1}{\ntime\nk}\sum_{n=1}^{\ntime}\sum_{k=1}^{\nk}\left(
            \dfrac{\hat{E}(n, k) - E(n, k)}{SD(k)}
            \right)^2} \, ,
    \label{eq:ke_nrmse}
\end{equation}\end{linenomath*}
where $\nk$ is the number of spectral coefficients and $SD(k)$ is the temporal
standard deviation of each spectral coefficient throughout the test trajectory.
As in \cref{sec:nvar-results}, all distributions and lineplots indicate
prediction skill from 50 randomly selected initial conditions from an unseen
test dataset.


\subsection{Soft Constraints on Spectral Error}
\label{subsec:esn-ego}

% Here we present ESN results, using mainly the two metrics used to evaluate
% NVAR
It is well known that ESN prediction skill is highly dependent on the global or
``macro-scale'' parameters noted in
\cref{eq:rc-hyperparameters},
\citep<$\esnparams$, e.g.>[]{platt_systematic_2022,lukosevicius_practical_2012}.
Following the success of previous studies in using Bayesian Optimization methods
to systematically tune these parameters
\citep{griffith_forecasting_2019,penny_integrating_2022,platt_systematic_2022},
we use the Bayesian Optimization algorithm outlined by \citet{jones_efficient_1998} and implemented by
\citet{bouhlel_python_2019} to find optimal parameter values.

More recently, \red{Platt et al.}\todo{Cite Jason}
showed that constraining these macro-scale
parameters using global invariant properties of the underlying system leads the
optimization algorithm to select parameters that generalize well to unseen test data. In that work, the authors were successful in using the largest positive
Lyapunov exponent, and to a lesser extent the fractal dimension of the system.
Because of the focus on resolved scales in this work, we take a similar approach, but test the effect of constraining the ESN to the KE density spectral coefficients.
Specifically, we implement the following two-stage training process.
At each step, the macro-scale parameters, $\esnparams$, are fixed, and the
``micro-scale'' parameters $\Wout$ are obtained by minimizing \cref{eq:cost}.
This readout matrix is then used to make forecasts from randomly selected
initial conditions from a validation dataset.
The skill of each of these forecasts is captured by the macro-scale cost
function
\begin{linenomath*}\begin{equation}
    \cf_\text{macro}(\esnparams) = \dfrac{1}{\nmacro}
    \sum_{j=1}^{\nmacro}
    \left\{
        \text{NRMSE}(j) + \gamma \text{KE\_NRMSE}(j)
    \right\}
    \label{eq:macro-cost} \, ,
\end{equation}\end{linenomath*}
where NRMSE and KE\_NRMSE are defined in \cref{eq:total-nrmse,eq:ke_nrmse},
$\nmacro$ is the number of forecasts used in the validation set, and $\gamma$ is
a hyperparameter that determines how much to penalize deviations
from the true KE density spectrum.
The value of $\macrocost$ is then used within the Bayesian Optimization algorithm, which reiterates the whole optimization process with new values for
$\esnparams$ until an optimal value is found or the maximum number of iterations is reached.
Here, we use $\nmacro=10$, initialize the optimization with 20~randomly sampled points in the 5~dimensional parameter space, and run for 10~iterations. Note that we run this optimization procedure for each unique ESN configuration
throughout \cref{sec:esn-results} (i.e., for each $\nsub$ and each $\gamma$
value).

\cref{fig:rc_qualitative_nsub01} shows a qualitative view of how penalizing the
KE density impacts ESN prediction skill when it operates at the original
timestep of the SQG model (i.e., $\nsub=1$).
At $\gamma=0$, the ESN parameters are selected based on NRMSE alone, and
the prediction is relatively blurry.
However, as $\gamma$ increases to $10^{-1}$, the prediction becomes sharper as
the small scale features are better resolved.

\cref{fig:rc_quantiative_nsub01} gives a quantitative view of how the KE density
penalty changes ESN prediction skill, once again with $\nsub=1$.
The first two panels show that there is a clear tradeoff between NRMSE and KE error:
as $\gamma$ increases the NRMSE increases but the spectral representation improves.
The final panel in \cref{fig:rc_quantiative_nsub01}
shows that the spatial scales at which the spectral error manifests in these
different solutions.
When $\gamma=0$, the macro-scale parameters are chosen to minimize NRMSE,
leading to blurry predictions and a dampened spectrum at the higher wavenumbers,
especially for $|\mathbf{K}| > 2\cdot10^{-3}$~rad~km$^{-1}$.
We note that \citet{lam_graphcast_2022} report the same behavior when using a cost function that is purely based on mean-squared error.
On the other hand, when $\gamma = 10^{-1}$, the global parameters are chosen to
minimize both NRMSE and KE density error, where the latter treats all spatial
scales equally.
In this case, KE relative error is reduced by more than a factor of two and the
spectral bias at higher wavenumbers is much more muted.
We note that using larger values of $\gamma$ produces similar results to
$\gamma=10^{-1}$.

Of course, the tradeoff for the reduced spectral error is larger NRMSE, resulting
from slight mismatches in the position of small scale features in the forecast.
However, our purpose is to generate forecasts that are as representative
of the training data as possible.
Overly smoothed forecasts are not desirable, because this translates to losing local extreme values,
which are of practical importance in weather and climate.
Additionally, a key aspect of ensemble forecasting is that the truth remains a
plausible member of the ensemble \citep{kalnay_ensemble_2006}.
Therefore, representing the small scale processes, at least to some degree,
will be critical for integrating an
emulator into an ensemble based prediction system.

Finally, we note that there is some irreducible high wavenumber error,
which is most clearly seen by comparing the prediction skill to a persistent
forecast.
While the sample median NRMSE for each $\gamma$ value beats persistence, the
KE\_NRMSE is more than double, due to this error at the small spatial scales.
Ideally, our forecasts would beat persistence in both of these metrics, but
obtaining the ``realism'' in the small spatial scales necessary to
dramatically reduce this spectral error should be addressed in future work.

%However, as time progresses through the 12~hour forecast window,
%the KE relative error increases slightly at the larger spatial scales,
%$|\mathbf{K}| \lessapprox 10^{-3}$~rad~km$^{-1}$ (not shown),
%and this error is what causes NRMSE to be roughly 10\% higher in the case where
%$\gamma=10^{-1}$ than $\gamma=0$.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figures/rc_qualitative_gamma.jpg}
    \caption{
        One sample prediction from the test dataset, where each panel shows
        potential temperature in the truth (left) and subsequently for
        ESN predictions with parameters optimized using
        $\gamma$~=~\{0,~$10^{-2}$,~$10^{-1}$\} in \cref{eq:macro-cost}.
        Each panel shows the prediction at a forecast lead time of 4~hours,
        using the same initial conditions as in \cref{fig:nvar_qualitative}.
        As $\gamma$ increases from left to right, the prediction becomes sharper
        (i.e., less blurry).
        Here, the ESN is evaluated at the SQG model timestep, i.e., $\nsub$~=~1.
    }
    \label{fig:rc_qualitative_nsub01}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figures/rc_all_nsub01.pdf}
    \caption{
        Quantitative comparison of ESN predictions at $\nsub$~=~1 with macro-scale
        parameters
        chosen using different values of $\gamma$ in \cref{eq:macro-cost}.
        NRMSE (\cref{eq:total-nrmse}; left), KE\_NRMSE (\cref{eq:ke_nrmse};
        middle), and
        KE relative error (\cref{eq:ke_relerr}; right) highlight the tradeoff
        between minimizing NRMSE and spectral error: as $\gamma$ increases
        spectral error is reduced, but NRMSE increases.
        Note that the KE relative error is shown at 4~hours to provide
        direct comparison to the snapshots in \cref{fig:rc_qualitative_nsub01}.
        In each plot, the solid gray line indicates the median skill of a persistent
        forecast.
    }
    \label{fig:rc_quantiative_nsub01}
\end{figure}



\subsection{Temporal Subsampling}
\label{subsec:esn-subsampling}


The NVAR predictions shown in \cref{subsec:nvar-subsampling} indicate that
subsampling the training data systematically increases error at small spatial
scales.
However, the architecture was not specifically designed or constrained to
have a good spectral representation of the underlying dynamics.
On the other hand, the previous section (\cref{subsec:esn-ego})
showed that the spectral bias at high wavenumbers
can be reduced by optimizing the global ESN parameters
to the true KE density spectrum.
Given these two results, we explore the following question: does temporal subsampling still
increase spectral bias in the more general ESN framework, even when parameters
are chosen to minimize this bias?

\cref{fig:rc_qualitative_gamma0.1} and \cref{fig:rc_quantiative_gamma0.1}
show that even when the macro-scale parameters are chosen to prioritize the KE
density representation (i.e., $\gamma = 10^{-1}$ is fixed),
temporal subsampling does lead to an apparently inescapable spectral bias.
This effect is shown qualitatively in \cref{fig:rc_qualitative_gamma0.1},
where the predictions become
smoother as the temporal subsampling factor, $\nsub$, increases.
The effect is similar to what was seen with NVAR except the blurring effect is
less pronounced.
Quantitatively, \cref{fig:rc_quantiative_gamma0.1}(b) shows that as $\nsub$
increases, error in KE density spectrum generally increases, while panel (c) shows that this KE
error is concentrated in the small spatial scales,
$|\mathbf{K}| > 2\cdot10^{-3}$~rad~km$^{-1}$.
We note that the degree of spectral bias at $\nsub=16$ is smaller than what was
achieved with NVAR for the same $\nsub$ value, cf. \cref{fig:nvar_ke_vs_lag},
indicating that the optimization was successful in reducing the spectral bias.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figures/rc_qualitative_nsub.jpg}
    \caption{One sample prediction from the test dataset, exactly as in
        \cref{fig:rc_qualitative_nsub01}, except here $\gamma$~=~$10^{-1}$ is fixed, and
        the temporal subsampling factor is varied: $\nsub$~=~\{1,~4,~16\}.
        As the temporal subsampling factor increases, the small spatial scale
        features are lost and the prediction becomes blurrier.
    }
    \label{fig:rc_qualitative_gamma0.1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figures/rc_all_gamma0.1.pdf}
    \caption{Quantitative comparison of ESN predictions, showing
        NRMSE (left), KE\_NRMSE (middle), and KE relative error (right), exactly as in
        \cref{fig:rc_quantiative_nsub01}, except here $\gamma$~=~$10^{-1}$ is fixed,
        and the temporal subsampling factor is varied: $\nsub$~=~\{1,~4,~16\}.
        As the temporal subsampling factor increases, spectral errors increase.
        In each plot, the solid gray line indicates the median skill of a persistent
        forecast.
    }
    \label{fig:rc_quantiative_gamma0.1}
\end{figure}

Interestingly, there is little difference between NRMSE obtained by the ESNs at
different $\nsub$ values.
Additionally, \cref{fig:rc_quantiative_gamma0.0} shows that there is little
difference in both NRMSE and KE\_NRMSE when $\gamma=0$, i.e., when NRMSE is the only criterion
for parameter selection.
This result shows that NRMSE alone is not a good criterion for model selection, given
that we have shown success in reducing spectral errors by prioritizing the spectrum appropriately.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figures/rc_all_gamma0.0.pdf}
    \caption{Same as \cref{fig:rc_quantiative_gamma0.1}, except here $\gamma$~=~0,
        indicating that only NRMSE is penalized in the cost function.
        The error is relatively similar, indicating that NRMSE alone is a
        suboptimal penalty for model selection.
        In each plot, the solid gray line indicates the median skill of a persistent
        forecast.
    }
    \label{fig:rc_quantiative_gamma0.0}
\end{figure}

\subsection{Impact of the Hidden Layer Dimension}
\label{subsec:esn-size}

The dimension of the hidden layer, $\nhidden$, also known as the reservoir size, determines the
memory capacity available to the ESN
\citep{jaeger_echo_2001,lukosevicius_practical_2012}.
For systems with high dimensional input signals, it is crucial to use a sufficiently large hidden layer to afford the memory capacity necessary for accurate
predictions \citep{hermans_memory_2010}.
In all of the preceding sections we fixed $\nhidden=6,000$ for each local group,
where for reference each local group has an input dimension of
$\nlocalinputstate=200$ and an output dimension of $\nlocalstate=128$.
Here, we briefly address the effect of doubling the hidden layer dimension, while keeping the input and output dimensions constant, in order to test how
sensitive our conclusions are on this crucial hyperparameter.
Due to the computational expense of the parameter optimization discussed in
\cref{subsec:esn-ego}, we only perform this experiment for $\nsub=16$.

The impact of doubling $\nhidden$ on prediction skill is shown in
\cref{fig:esn-size}, where for the sake of brevity we only show results for the
case when $\gamma=10^{-1}$ in \cref{eq:macro-cost}.
The left panel shows that the larger hidden layer actually increases the NRMSE slightly.
However, the middle and right panels show that this increase is due to the improved
spectral representation.
The improvement in KE\_NRMSE is nearly proportional to the improvement achieved by increasing
the temporal resolution of the data.
That is, doubling the hidden layer width reduces the average KE\_NRMSE by 14\%,
while increasing the temporal resolution of the data by a factor of 4 reduces
the KE\_NRMSE by 30\%.
These results indicate a potential brute force approach to overcoming the
subsampling related spectral errors.
However, the larger hidden layer dimension has to be constrained
with enough training data, and requires more
computational resources.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figures/rc_reservoir_size.pdf}
    \caption{The impact of doubling the hidden layer dimension from
        $\nhidden$~=~6,000 to
        $\nhidden$~=~12,000 on NRMSE (left), KE\_NRMSE (middle), and KE relative
        error (right).
        Increasing the hidden layer dimension is relatively proportional to reducing
        the temporal subsampling factor, indicating a potential brute force
        approach to reducing the subsampling related spectral errors.
        Here $\gamma$~=~$10^{-1}$, and the solid gray line indicates the
        median skill of a persistent forecast.
    }
    \label{fig:esn-size}
\end{figure}

\subsection{Impact of Training Dataset Size}
\label{subsec:esn-fixed-steps}

In all of the preceding experiments, the length of training time was fixed to
15~years, meaning that there are fewer training samples when the data are
subsampled, i.e., as $\nsub$ grows.
Specifically, 15~years of data at an original model timestep of 5~minutes means
that there are approximately
$1.6\cdot10^{6}$, $3.9\cdot10^5$, and $9.72\cdot10^4$ samples
for each case previously shown: $\nsub=1$, 4, and 16, respectively.
Here, we show that even when the number of training samples is fixed, the
subsampling related spectral errors are still present.

\cref{fig:esn-fixed-steps} shows the prediction skill in terms of NRMSE and
spectral errors when the number of training samples is fixed to $9.72\cdot10^4$.
With this number of samples, the training data is exactly the same for
$\nsub=16$, but only spans $3.75$ and $0.94$~years for $\nsub=4$ and $\nsub=1$,
respectively.
However, we see the same general trend as before: subsampling the data improves
NRMSE slightly but increases the KE\_NRMSE.
As before, the spectral error is largest in the higher wavenumbers,
$|\mathbf{K}| > 2\cdot10^{-3}$~rad~km$^{-1}$.
We note that the difference in performance between $\nsub=4$ and $\nsub=16$ is
marginal.
The only notable difference between these two cases is that the ESN is less
consistent, i.e., the KE\_NRMSE distribution is broader, when $\nsub=16$.
However, it is clear that spectral error is lowest when the data are not
subsampled at all, even though less than a year of data is used.
This result indicates that there could be a benefit to training a RNN on a
relatively shorter model trajectory that is untouched, rather than a longer
dataset that is subsampled in time.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figures/rc_fixed_steps.pdf}
    \caption{Subsampling related spectral errors persist even when the number of
        training samples is fixed. Here, the number of samples is fixed to
        $9.72\times10^{4}$ for all cases, and yet the temporal subsampling
        related spectral errors remain.
        Here, $\gamma$~=~$10^{-1}$ and the solid gray line indicates the median skill of a persistent
        forecast.
    }
    \label{fig:esn-fixed-steps}
\end{figure}
