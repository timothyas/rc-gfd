\section{Single Layer Recurrent and Autoregressive Neural Networks}
\label{sec:rnn-architecture}

Our goal is to develop an emulator that can reproduce the time evolution of a
dynamical system, such that its future state can be predicted from an initial
state estimate.
Therefore we use the following generic, discrete-time equations for our
recurrent and autoregressive models,
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \hidden(n+1) &= \hiddenlayer\left(
            \hidden(n), \state(n); \hyperparameters
            \right) \\
        \hat{\state}(n+1) &= \outputlayer \left( \hidden(n+1) \right) \, ,
    \end{aligned}
    \label{eq:rnn}
\end{equation}\end{linenomath*}
as in \citet{goodfellow_sequence_2016}.
Here $n\in\mathbb{Z}$ denotes a particular timestep $t = n\Delta \tau$,
where $\Delta\tau = \nsub\Delta t$ is the timestep size of the neural network,
which may be larger than $\Delta t=5$~minutes, the step size of the original model described in
\cref{sec:sqg}.
Here
$\state(n)\in\statespace$ is the state of the dynamical system and
$\hidden(n)\in\hiddenspace$ is the hidden or internal state of the network,
which is also referred to as the ``reservoir'' in RC or ``feature vector'' in
NVAR.
The generic function $\hiddenlayer(\cdot)$ evolves this hidden state forward in
time subject to the explicit
influence of the current hidden and system states, as well as the macro-scale
parameters $\hyperparameters$.
The output layer,
$\outputlayer(\cdot)$, or ``readout'' operation,
maps the hidden state back to the original state space, giving an
approximation of the target system.

During the training phase, $\state(n)$ is provided to the model at each timestep
and the misfit between the approximation and data,
$\hat{\state}(n+1) - \state(n+1)$, is used to train the weights in the output
layer.
After training, during the prediction phase, the network becomes an autonomous
system:
\begin{linenomath*}\begin{equation*}
    \hidden(n+1) = \hiddenlayer\left(
        \hidden(n), \hat{\state}(n); \hyperparameters\right) \, .
\end{equation*}\end{linenomath*}
%\todo{mention spinup ...}

The neural network architectures that we use employ common
simplifications that are relevant to the readout operator and training
procedure; we discuss these simplifications in \cref{subsec:readout}.
Additionally, we employ a similar strategy to parallelize the architecture for
high dimensional systems, and this is discussed in
\cref{subsec:parallelization}.
Finally, the specific form of $\hiddenlayer(\cdot)$ for the ESN and NVAR architectures
is provided in \cref{subsec:rc,subsec:nvar},
respectively.


\subsection{Simplified Readout and Training}
\label{subsec:readout}

The neural networks that we use employ two
simplifications relative to the generic form presented in
\cref{eq:rnn}.
First, any internal relationships encapsulated within
$\hiddenlayer(\cdot)$ are pre-defined by the macro-scale parameters,
$\hyperparameters$.
Therefore, no internal weights contained within $\hiddenlayer(\cdot)$
are learned during the formal training process.
Secondly, the readout operator is linear, such that
\begin{linenomath*}\begin{equation*}
    \outputlayer(\hidden(n)) \coloneqq \Wout \hidden(t) \, ,
\end{equation*}\end{linenomath*}
where $\Wout \in \Woutspace$ is a matrix.
The result of these two assumptions is a cost function that is quadratic with
respect to the elements of $\Wout$,
\begin{linenomath*}\begin{equation}
    \cf(\Wout) =
        \dfrac{1}{2\ntrain}\sum_{n=1}^{\ntrain}
        \norm{\Wout \hidden(n) - \state(n)}^2_2
        +
        \dfrac{\tikhonov}{2}\norm{\Wout}_\text{F}^2 \, .
    \label{eq:cost}
\end{equation}\end{linenomath*}
Here
$\norm{\mathbf{A}}_\text{F} \coloneqq
\sqrt{\text{Tr}\left(\mathbf{A}\mathbf{A}^T\right)}$
is the Frobenius norm,
$\ntrain$ is the number of time steps used for training,
$\tikhonov$ is a Tikhonov regularization parameter \citep{tikhonov_solution_1963}, chosen to improve
numerical stability and prevent overfitting.

The hidden and target states can be expressed in matrix form by concatenating
each time step ``column-wise'':
$\Hidden \coloneqq (\hidden(1) \, \hidden(2) \, \cdots \, \hidden(\ntrain))$,
and similarly\\
\noindent$\State \coloneqq (\state(1) \, \state(2) \, \cdots \, \state(\ntrain))$.
With this notation, the elements of $\Wout$ can be compactly written as the
solution to the linear ridge regression problem
\begin{linenomath*}\begin{equation}
    \Wout = \State \Hidden^T \left(\dfrac{1}{\ntrain}\Hidden\Hidden^T +
    \tikhonov\mathbf{I}\right)^{-1} \, ,
    \label{eq:ridge_regression}
\end{equation}\end{linenomath*}
although we do not form the inverse explicitly.
We instead use the \texttt{solve} function from SciPy's linear algebra module
\citep{scipy_2020}, based on testing by
\citet{platt_systematic_2022}.


\subsection{Parallelization Strategy}
\label{subsec:parallelization}

The model architectures that we use inherit the gridded structure of the target
state being emulated, and often require hidden states that are
$\mathcal{O}(10-100)$ larger.
Atmosphere and ocean GCMs typically propagate high dimensional state vectors,
$\mathcal{O}(>10^6)$,
so representing the system with a single hidden state would be intractable.
Thus, we employ a parallelization strategy to distribute the target and hidden
states across many semi-independent networks.
Our strategy follows the algorithm introduced by \citet{pathak_model-free_2018},
and follows a similar construction as \citet{arcomano_machine_2020}.
We outline the procedure here and note an illustration of the process for
the ESN architecture in \cref{fig:esn-diagram}.

We subdivide the domain into $\ngroups$ rectangular groups based on horizontal location,
akin to typical domain decomposition techniques for atmosphere and ocean
GCMs on structured grids.
Each group contains
$N_x\times N_y$ horizontal grid cells, and all $\nvertical$
vertical grid cells at each horizontal location.
The global state vector, $\state$, which consists of all state variables to be
emulated at all grid cells, is partitioned into $\ngroups$ local state vectors,
$\localstate$.
For example, \cref{fig:esn-diagram} shows a field $\state$ decomposed into nine
groups, where each group is delineated by white lines.

In order to facilitate interactions between nearby groups, each group
has a designated overlap region which consists of $\noverlap$ elements
from its neighboring groups.
The local group and overlapping points are illustrated in \cref{fig:esn-diagram}
with a black box.
The local state vectors, plus elements from the overlap region, are concatenated
to form local input state vectors, $\localinputstate$.
These local input vectors drive separate networks at each group, thereby generating
distinct hidden states for each group as follows
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \localhidden(n+1)
        &= \hiddenlayer\left(
            \localhidden(n), \localinputstate(n); \hyperparameters
        \right) \\
        \localoutput(n+1)
        &= \localWout \localhidden(n+1) \, .
    \end{aligned}
    \label{eq:local-rnn}
\end{equation}\end{linenomath*}
We make the assumption that the hyperparameters which determine internal
connections within $\hiddenlayer(\cdot)$ are globally fixed.
Therefore, the only components
that drive unique hidden states in each group are the local input vector
$\localinputstate$ and the local readout matrix, $\localWout$.

During the training phase, each group acts completely independently from one
another.
Therefore, the training process is embarrassingly parallel and allows us to
scale the problem to arbitrarily large state vectors across a distributed
computing system, subject to resource constraints.
During the prediction phase, neighboring elements must be passed between
groups in order to fill each overlap region at each time step.
%We note that this requirement is not overbearing, as GCMs require similar
%message passing between neighboring processes in order to compute spatial
%derivatives and perform interpolation.

\subsection{Nonlinear Vector Autoregression Design}
\label{subsec:nvar}

%In \citet{gauthier_next_2021} this RNN is referred to as ``Next Generation
%Reservoir Computing'' because it is motivated by the RC framework.
%However, we note (as do the authors), that this is identical to the already
%existing Nonlinear Vector Autoregression (NVAR), and so we refer to this general architecture as
%such.
As in \citet{gauthier_next_2021,chen_next_2022}, we form the hidden state using
polynomial combinations of the time-lagged input state.
We explain this process with a simple example using a two variable system,
$\inputstate(n) = [u_0(n), u_1(n)]^T$,
a maximum polynomial degree,
$\maxpolynomial=2$, and a generic maximum number of lagged states, $\maxlag$:
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \localhidden(n+1)
        =
        [&1, \\
        % linear
         &u_0(n), \,\, u_1(n), \,\,
        u_0(n-1),\,\, u_1(n-1), \,\,
        \cdots \,\,
        u_0(n-\maxlag), \,\, u_1(n-\maxlag), \\
        % quadratic
         &u_0^2(n), \,\, u_1^2(n), \,\, u_0(n)u_1(n), \,\,
        u_0^2(n-1), \,\, \cdots \,\, u_1^2(n-\nlag) \\
         &u_0(n)u_0(n-1), \,\,
        u_0(n)u_1(n-1), \,\, \cdots \,\, u_0(n-\nlag)u_1(n), \,\,\cdots
        ] \\
        \localoutput(n+1) = &\localWout \localhidden(n+1) \, .
    \end{aligned}
\end{equation}\end{linenomath*}
Clearly, the size of the hidden state vector grows rapidly with
$\maxpolynomial$ and $\nlag$,
even for relatively low dimensional systems
\citep<see supplemental material of>[for explicit calculations]{chen_next_2022}.
We therefore make a simplification to the generic polynomial NVAR model.
That is, we only represent nonlinear interactions between points that lie
within a given radius between one another, defined by the number of neighboring
points, $\nneighbor$.
As a simple example, with $\nneighbor=1$ and $\maxlag=0$, the quadratic elements of a periodic, four variable
system would be
\begin{linenomath*}\begin{equation*}
    u_0^2, \,\, u_1^2, \,\, u_2^2, \,\, u_3^2, \,\,
    u_0u_1, \,\, u_0u_3, \,\, u_1u_2, \,\, u_2u_3
\end{equation*}\end{linenomath*}
ignoring ``non-local'' interactions such as $u_0u_2$.
In order to make this parameter consistent with the overlap region in the
parallelization scheme (\cref{subsec:parallelization}),
we set $\nneighbor = \noverlap = 1$ \red{SEE TABLE}.

All of the remaining macro-scale parameters that determine the NVAR performance are
\begin{linenomath*}\begin{equation*}
    \nvarparams =
    \{ \maxpolynomial, \maxlag, \tikhonov \} \, .
\end{equation*}\end{linenomath*}
By using the preconditioning scheme introduced in \citet{chen_next_2022},
we found results to be insensitive to the Tikhonov parameter $\tikhonov$, and so
we fix this to $\tikhonov = 10^{-4}$.
As noted earlier, we set $\maxpolynomial = 2$.
Our assumption behind this decision is that the NVAR model will be able to learn local
quantities like gradients and fluxes between neighboring grid cells.
Based on the results from \citet{chen_next_2022},
the NVAR model should then be able to use this information to construct
arbitrarily complex time stepping schemes as a function of $\nlag$.
Because of its explicit nature, we manually vary this
integer parameter to understand how memory impacts NVAR prediction skill.


\subsection{Echo State Network Design}
\label{subsec:rc}


\begin{figure}
    \centering
    \begin{overpic}[width=.8\textwidth]{../figures/esn-diagram.pdf}

        \put(25, 40) {\footnotesize $\localinputstate(n)$}
        \put(32.5, 31) {\footnotesize $\inputmatrix$}

        \put(34, 18) {\footnotesize $\adjacency$}
        \put(18.5, 11) {\footnotesize$\localhidden(n)$}

        \put(47, 17) {\footnotesize $\tanh(\cdot)$}
        \put(54.5, 22) {\footnotesize$\alpha$}
        \put(56, 6) {\footnotesize $1-\alpha$}

        \put(55, 32) {\footnotesize $\localhidden(n+1)$}
        \put(62.75, 20) {\footnotesize $\localWout$}
        \put(65, 29) {\footnotesize $\localoutput(n+1)$}
    \end{overpic}
    \caption{An illustration of the ESN architecture used, as it
        is applied to each local group throughout the domain.
        The domain is decomposed purely based on horizontal location, so the
        illustration shows a single horizontal slice, but note that each group
        contains all $\nvertical$ vertical levels.
        In this example, there are nine groups delineated by the white lines on
        the 2D slice on the left.
        The black box denotes the group being operated on, which includes a
        region of width $\noverlap$ that overlaps with neighboring groups.
        At timestep $n$, the group is flattened to make the input vector
        $\localinputstate(n)$, which is
        mapped into the ESN via $\inputmatrix$.
        The output $\localoutput(n+1)$ is expanded to fill its position in the global
        domain.
    }
    \label{fig:esn-diagram}
\end{figure}

Our ESN architecture is illustrated in \cref{fig:esn-diagram}, and is defined as
follows
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \localhidden(n+1)
        &=
        \left(1-\leak\right)\localhidden(n)
        +
        \leak \tanh\left(
            \adjacency \localhidden(n) + \inputmatrix \localinputstate(n) + \biasvector
            \right)
             \\
        \localoutput(n+1)
        &= \localWout \localhidden(n+1) \, .
    \end{aligned}
    \label{eq:rc}
\end{equation}\end{linenomath*}
Here
$\leak\in[0,1]$ is a leak parameter,
$\adjacency \in \mathbb{R}^{\nhidden\times\nhidden}$ is an adjacency matrix that
determines the internal connections between the nodes of the hidden state,
$\inputmatrix \in \mathbb{R}^{\nhidden\times\ninputstate}$ maps the input vector
into the higher dimensional hidden state,
and $\biasvector\in\mathbb{R}^{\nhidden}$
is the bias vector with elements
$\biasvectorelement_i \sim \mathcal{U}(-\bias,\bias)$.
Finally, we note that ESNs require a spinup period before generating
predictions, so we specify a 10~day spinup period for all validation and testing
samples.
%Given the parallelization scheme described in \cref{subsec:parallelization},
%the network can be considered as $\ngroups$ interacting ESNs, each with
%$\nhidden$ nodes, or as one network with $\ngroups\times\nhidden$ nodes.

Two scalar parameters, $\spectralradius$ and $\inputscaling$,
are used to control the scaling of the adjacency and input matrices,
respectively.
These parameters have dramatic influence on the ESNs prediction skill, since their
values influence the network's memory and stability
\citep{lukosevicius_practical_2012,hermans_memory_2010}.
Here we first normalize the matrices by their largest singular value, and then
apply the scaling parameters as follows
\begin{linenomath*}\begin{equation*}
    \adjacency \coloneqq
    \dfrac{\spectralradius}{\sigma_{max}\left(\hat{\adjacency}\right)}
    \hat{\adjacency}
    \qquad
    \inputmatrix \coloneqq
    \dfrac{\sigma}{\sigma_{max}\left(\hat{\mathbf{W}}_\text{in}\right)}
    \hat{\mathbf{W}}_\text{in} \,
\end{equation*}\end{linenomath*}
where the elements of $\hat{\mathbf{W}}_\text{in}$
are initialized with elements $\hat{w}_{i,j}\sim\mathcal{U}(-1,1)$.
The initial adjacency matrix is generated similarly, except that the indices
$i,j$ are randomly chosen such that $\hat{\adjacency}$ attains a specified
sparsity.
Here we set the matrix sparsity to $1 - \kappa / \nhidden$, with $\kappa=6$,
following the success of very sparsely connected adjacency matrices as shown by
\citet{griffith_forecasting_2019}.
By first normalizing the matrices by the largest singular value, the parameters
$\spectralradius$ and $\inputscaling$ re-scale the induced 2-norm of
the matrix.
This normalization is not standard in the ESN literature, but we found that it
helped improve prediction skill.
We provide further discussion of this process in \cref{sec:new_methods}.

In summary, the macro-scale parameters that determine the overall
characteristics of the ESN are

\begin{linenomath*}\begin{equation}
    \esnparams =
    \{ \spectralradius, \inputscaling, \bias, \leak, \tikhonov \} \,
    ,
    \label{eq:rc-hyperparameters}
\end{equation}\end{linenomath*}
which are globally fixed for all groups.
Due to the high sensitivity of ESN prediction skill to these parameter values,
we follow the general optimization framework described by
\citet{platt_systematic_2022} to determine approximately optimal values.
We use the Bayesian optimization algorithm outlined by
\citet{jones_efficient_1998} and implemented by \citet{bouhlel_python_2019} to tune them.
This process is discussed in \cref{sec:esn-results}, \red{and more details are
provided in the supplement}.
