\section{Forecasting with Recurrent Neural Networks}
\label{sec:rnn-architecture}

Our goal is to develop an emulator that can reproduce the time evolution of a
dynamical system, such that its future state can be predicted from an initial
state estimate.
RNNs are a family of NNs that process information as a sequence,
such as the chronological ordering time,
and we therefore consider them to be a natural choice for the forecasting
task.
Following \citet{goodfellow_sequence_2016}, we use the following discrete-time form
for RNNs in this work:
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \hidden(n+1) &= \hiddenlayer\left(
            \hidden(n), \state(n); \hyperparameters
            \right) \\
        \hat{\state}(n+1) &= \outputlayer \left( \hidden(n+1) \right) \, ,
    \end{aligned}
    \label{eq:rnn}
\end{equation}\end{linenomath*}
where $n\in\mathbb{Z}$ denotes the a particular point in time $t = n\Delta t$,
given the time step size $\Delta t$.
Here
$\hidden(n)\in\hiddenspace$ is the hidden or internal state of the RNN,
and
$\state(n)\in\statespace$ is the state of the dynamical system.
The generic function $\hiddenlayer(\cdot)$ evolves this hidden state forward in
time subject to the explicit
influence of the current hidden and system states, and the hyperparameters
$\hyperparameters$.
The output layer,
$\outputlayer(\cdot)$, or ``readout'' operation,
maps the projected hidden state back to the original state space, giving an
approximation of the target system.

During the training phase, $\state(n)$ is provided to the RNN at each time step
and the misfit between the approximation and data,
$\hat{\state}(n+1) - \state(n+1)$ is used to train the weights in the output
layer.
After training, during the prediction phase, the RNN becomes an autonomous
system as the hidden layer then becomes
\begin{linenomath*}\begin{equation*}
    \hidden(n+1) = \hiddenlayer\left(
        \hidden(n), \hat{\state}(n); \hyperparameters\right) \, .
\end{equation*}\end{linenomath*}
\todo{mention spinup ...}

The RNN architectures that we use in this work employ common
simplifications that are relevant to the readout operator and training
procedure; we discuss this in \cref{subsec:readout}.
Additionally, we employ a similar strategy to parallelize the architecture for
high dimensional systems, and this is discussed in
\cref{subsec:parallelization}.
Finally, the specific form of $\hiddenlayer(\cdot)$ for the RC and NVAR architectures
is provided in \cref{subsec:rc,subsec:nvar},
respectively.


\subsection{Simplified Readout and Training}
\label{subsec:readout}

The RNN architectures that we use in this study employ two
simplifications relative to the generic form presented in
\cref{eq:rnn}.
First, any internal connections between the nodes of the hidden state,
$\hidden(n)$, are pre-defined by the model hyperparameters.
That is, no internal weights contained within $\hiddenlayer(\cdot)$
are learned during the training process.
Secondly, the readout operator is linear, such that
\begin{linenomath*}\begin{equation*}
    \outputlayer(\hidden(n)) \coloneqq \Wout \hidden(t) \, ,
\end{equation*}\end{linenomath*}
where $\Wout \in \Woutspace$ is a matrix.
The result of these two assumptions is a cost function that is quadratic with
respect to the elements of $\Wout$.

To be more precise, the goal of the training process is to find the matrix
elements which minimize the regularized, model-data misfit cost function:
\begin{linenomath*}\begin{equation}
    \cf(\Wout) =
        \dfrac{1}{2}\sum_{n=1}^{\ntrain} \norm{\Wout \hidden_n - \state_n}^2
        +
        \dfrac{\tikhonov}{2}\norm{\Wout}_\text{F}^2 \, .
    \label{eq:cost}
\end{equation}\end{linenomath*}
Here
$\norm{\mathbf{A}}_\text{F} \coloneqq
\sqrt{\text{Tr}\left(\mathbf{A}\mathbf{A}^T\right)}$
is the Frobenius norm,
$\ntrain$ is the number of time steps used for training,
$\tikhonov$ is a Tikhonov regularization parameter \citep{tikhonov_solution_1963}, chosen to improve
numerical stability and prevent overfitting.

The hidden and target states can be expressed in matrix form by concatenating
each time step ``column-wise'':
$\Hidden \coloneqq (\hidden_1 \, \hidden_2 \, \cdots \, \hidden_{\ntrain})$,
and similarly
$\State \coloneqq (\state_1 \, \state_2 \, \cdots \, \state_{\ntrain})$.
With this notation, the elements of $\Wout$ can be compactly written as the
solution to the linear ridge regression problem
\begin{equation}
    \Wout = \State \Hidden^T \left(\Hidden\Hidden^T + \tikhonov\right)^{-1} \, ,
    \label{eq:ridge_regression}
\end{equation}
although we do not form the inverse explicitly.
We instead use the \texttt{solve} function from SciPy's linear algebra module
\citep{scipy_2020}, based on performance and efficiency testing by
\citet{platt_systematic_2022}. \todo{talk about python implementation?}


\subsection{Parallelization Strategy}
\label{subsec:parallelization}

The RNN architectures that we use inherit the gridded structure of the target
state being emulated, and require hidden states that are
$\mathcal{O}(10-100)$ larger.
Atmosphere and ocean GCMs typically propagate high dimensional state vectors,
$\mathcal{O}(>10^6)$,
so representing the system with a single hidden state would be intractable.
Thus, we employ a parallelization strategy to distribute the target and hidden
states across many semi-independent RNNs.
Our strategy follows the algorithm introduced by \citet{pathak_model-free_2018},
and follows the same general construction as \citet{arcomano_machine_2020}.
We outline the procedure here and note an illustration of the process in
\red{FIGURE}.

We subdivide the domain into $\ngroups$ rectangular groups based on horizontal location,
akin to typical domain decomposition for atmosphere and ocean
GCMs on structured grids \red{CITE?}.
Each group contains
$\nlocalstate\times\nlocalstate$ horizontal grid cells, and all $\nvertical$
vertical grid cells at each horizontal location.
The global state vector, $\state$, which consists of all state variables to be
emulated at all grid cells, is partitioned into $\ngroups$ local state vectors,
$\localstate$, where $i$ and $j$ denote the relative position of the group in
each horizontal direction.
In order to facilitate interactions between nearby groups, each group
has a designated overlap region which consists of the $\noverlap$ elements
from its neighboring groups.
The local state vectors, plus elements from the overlap region, are concatenated
to form local input state vectors, $\localinputstate$.
These local input vectors drive separate RNNs at each group, thereby generating
distinct hidden states for each group as follows
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \localhidden(n+1)
        &= \hiddenlayer\left(
            \localhidden(n), \localinputstate(n); \hyperparameters
        \right) \\
        \localoutput(n+1)
        &= \localWout \localhidden(n+1) \, .
    \end{aligned}
    \label{eq:local-rnn}
\end{equation}\end{linenomath*}
We make the assumption that the hyperparameters which determine internal
connections within $\hiddenlayer(\cdot)$ are globally fixed.
Therefore, the only components
that drive unique hidden states in each groups are the local input vector
$\localinputstate$ and the local readout matrix, $\localWout$.

During the training phase, each group acts completely independently from one
another.
Therefore, the training process is embarrassingly parallel and allows us to
scale the problem to arbitrarily large state vectors across a distributed
computing system, subject to resource constraints.
During the prediction phase, neighboring elements must be passed between
groups in order to fill each overlap region at each time step.
We note that this requirement is not overbearing, as GCMs require similar
message passing between neighboring processes in order to compute spatial
derivatives and perform interpolation.


\subsection{Reservoir Computing}
\label{subsec:rc}
RC refers to a broad class of RNN architectures developed in the early
2000s, where it was independently discovered as
ESNs
LSMs
DCBP.
In our work we focus on the ESN perspective, and note the work of
\citet{verstraeten_experimental_2007}, who presented a unification of
these perspectives within the broader RC paradigm.
One defining characteristic of RC is that all internal connections are
preset by the hyperparameters, and only the output layer is trained
(\cref{subsec:readout}).
This structure significantly reduces the number of parameters that need to be
trained, as the internal connections are now governed by a handful of
``macro-scale'' parameters that govern the entire internal network.
Additionally, fixing the internal connections side-steps issues with exploding
or vanishing gradients during backpropagation - a problem that plagued RNNs
\red{CITE}
until the introduction of long short-term memory (LSTM) networks.

The relatively simplified structure and training requirements of RC makes it an
attractive architecture for large scale prediction because it enables rapid development.
More importantly though, we are motivated to use RC because it has repeatedly
shown excellent prediction skill for low dimensional chaotic systems
\citep<e.g.>[]{platt_systematic_2022,vlachas_backpropagation_2020,griffith_forecasting_2019,lu_attractor_2018,pathak_model-free_2018},
and comparable prediction skill of the atmospheric state to a coarse resolution
primitive equation GCM \citep{arcomano_machine_2020}.
Here we summarize a few key results.
\citet{platt_systematic_2022} showed that by systematically optimizing the RC
hyperparameters, it can emulate the dynamics of a variety of chaotic systems and
remain on the ``true'' trajectory out to 4-12 times $1/\lambda_1$ on average, where
$\lambda_1$ is the leading Lyapunov exponent.
\red{PATHAK 2017} and \citep{lu_attractor_2018} showed that when an RC model
eventually diverges from the ``truth'' in a chaotic system, the long-term behavior
resembles a typical trajectory on the attractor, such that RC respects the
ergodic properties of the underlying system.
Moreover, \citet{vlachas_backpropagation_2020} showed that RC can outperform more
complex RNNs like LSTMs in predicting chaotic dynamics, indicating that the
fixed internal connections could afford some robustness for prediction.
Finally, we note that \citep{penny_integrating_2022} showed that RC can be
successfully integrated into a number of data assimilation algorithms, either
by generating samples for ensemble based methods like the Ensemble Kalman Filter,
or by generating the tangent linear model necessary for 4D-Var.
All of this work motivates our choice to use RC as an emulator for turbulent
geophyiscal fluid dynamics in the context of weather forecasting.

While many variants exist, we employ the following RC structure
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \localhidden(n+1)
        &=
        \left(1-\leak\right)\hidden(n)
        +
        \leak \tanh\left(
            \adjacency \hidden(n) + \inputmatrix \inputstate(n) + \bias\ones
            \right)
             \\
        \localoutput(n+1)
        &= \localWout \localhidden(n+1) \, .
    \end{aligned}
    \label{eq:rc}
\end{equation}\end{linenomath*}
Here
$\leak\in\mathbb{R}$ is a leak parameter,
$\adjacency \in \mathbb{R}^{\nhidden\times\nhidden}$ is an adjacency matrix that
determines the internal connections between the nodes of the hidden state,
$\inputmatrix \in \mathbb{R}^{\nhidden\times\ninputstate}$ maps the input vector
into the higher dimensional hidden state,
and $\bias\in\mathbb{R}$ together with $\ones\in\mathbb{R}^{\nhidden}$
sets the bias.
For clarity, we can consider the RNN as being constructed by $\ngroups$ separate
networks, each with $\nhidden$ nodes.
Alternatively, this structure could be considered a single RNN with
$\ngroups\times\nhidden$ total nodes.

Two additional hyperparameters that control the scaling of $\adjacency$ and
$\inputmatrix$ are implicitly included in \cref{eq:rc}.
First, the adjacency matrix is initialized with normally distributed random weights,
then normalized by its spectral radius, and scaled by the desired spectral radius
$\spectralradius$.
Secondly, the input matrix is initialized with normally distributed random
weights, ranging from $[-\inputscaling, \inputscaling]$, based on the scaling
parameter $\inputscaling \in \mathbb{R}$.
We note that some forms of RC choose to add additional nonlinearity to the
output layer, for instance by squaring every other entry of $\localhidden$
\citep<e.g.>[]{arcomano_machine_2020}.
However, we choose a linear readout based on results by
\citet{platt_systematic_2022}, who showed that prediction skill is insensitive
to this choice for a variety of dynamical systems.

We follow the general optimization framework described by
\citet{platt_systematic_2022} to determine the hyperparameters
\begin{linenomath*}\begin{equation*}
    \hyperparameters_\text{RC} =
    \{ \spectralradius, \inputscaling, \bias, \leak, \noverlap, \tikhonov \} \,
    ,
\end{equation*}\end{linenomath*}
where the hyperparameters is fixed for all groups.
We use the Bayesian optimization algorithm \red{CITE}
to tune each of the hyperparameters.
Do to the high dimensionality of the problems, we use relatively few iterations
to calibrate the model.
See \red{TABLE} for tabulated parameters.\todo{Tighten this up at the end}

\subsection{Nonlinear Vector Auto-Regression}
\label{subsec:nvar}

Recently, \citet{gauthier_next_2021} proposed a further simplification to the RC
paradigm based on insights from \citet{bollt_explaining_2021}.
In \citet{bollt_explaining_2021} it is shown that the hidden state, or
``reservoir'', in the traditional ESN can be interpreted as a linear combination
of time-delayed system states.
With this intuition, \citet{gauthier_next_2021} showed that the hidden state, or
``feature vector'' vector can be formed as polynomial combinations of the
time-lagged input state.
This form has shown excellent prediction skill for chaotic dynamical systems
\citep{barbosa_learning_2022,gauthier_next_2021}, despite requiring a much
smaller hidden state and less training data.
Additionally, because the hidden state is formed as explicit functions of the
target system, it is more straightforward to interpret than other RNNs.
For example, \red{CITE-TC} showed for a polynomial-based hidden state, a
well-trained readout matrix $\Wout$ directly reconstructs the numerical integration scheme of
the underlying training data, providing unprecedented prediction skill for
chaotic systems.


In \citet{gauthier_next_2021} this RNN is referred to as ``Next Generation
Reservoir Computing'' because it is motivated by the RC framework.
However, we note that this formulation is simply a new perspective on Nonlinear
Vector Auto-regression (NVAR), and so we refer to this general architecture as
such \red{DEFINITELY NEED TO CITE}.
As in \citet{gauthier_next_2021}\red{TC}, we form the hidden state using
polynomial combinations of the time-lagged input state.
We explain this process with a simple example using a two variable system,
$\inputstate(n) = [u_0(n), u_1(n)]^T$,
a maximum polynomial degree,
$\maxpolynomial=2$, and a generic maximum number of lagged states, $\maxlag$:
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \localhidden(n+1)
        =
        [&1, \,\,
        % linear
        u_0(n), \,\, u_1(n), \,\,
        u_0(n-1),\,\, u_1(n-1), \,\,
        \cdots \,\,
        u_0(n-\maxlag), \,\, u_1(n-\maxlag), \\
        % quadratic
         &u_0^2(n), \,\, u_1^2(n), \,\, u_0(n)u_1(n), \,\,
        u_0^2(n-1), \,\, \cdots \\
         &u_0(n)u_0(n-1), \,\, \cdots \,\,
        u_0(n)u_1(n-1), \,\, \cdots
        ] \\
        \localoutput(n+1) = &\localWout \localhidden(n+1) \, .
    \end{aligned}
\end{equation}\end{linenomath*}
We note that the hidden state can be formed using other functions e.g., radial
basis functions \red{CITE}, and this can be pursued in future work.

Clearly, the size of the hidden state vector grows rapidly with $p$ and $k$,
even for relatively low dimensional systems \red{SEE SUPPLEMENT TC for some
explicit numbers}.
We therefore make a simplification to the generic polynomial NVAR machine.
That is, we only consider nonlinear interactions between a certain local radius
around each point, defined by the number of neighbors parameter $\nneighbor$.
To be explicit, with $\nneighbor=1$, the quadratic elements of a periodic, four variable
system would be
\begin{linenomath*}\begin{equation*}
    u_0^2, \,\, u_1^2, \,\, u_2^2, \,\, u_3^2, \,\,
    u_0u_1, \,\, u_0u_3, \,\, u_1u_2, \,\, u_2u_3
\end{equation*}\end{linenomath*}
ignoring ``non-local'' interactions such as $u_0u_2$.
In order to make this parameter consistent with the overlap region in the
parallelization scheme (\cref{subsec:parallelization}),
we set $\nneighbor = \noverlap$.

All hyperparameters for this form of NVAR are
\begin{linenomath*}\begin{equation*}
    \hyperparameters_\text{NVAR} =
    \{ \maxpolynomial, \maxlag, \noverlap, \tikhonov \} \, .
\end{equation*}\end{linenomath*}
Throughout this work, we set $\maxpolynomial = 2$.
We note that by using the preconditioning scheme introduced in \red{CITE-TC},
we found results to be insensitive to the Tikhonov parameter $\tikhonov$, and so
we fix this to $\tikhonov = 10^{-4}$.
Rather than optimize the remaining hyperparameters,
$\maxpolynomial$, $\noverlap$, and $\maxlag$, we opt to tune
these manually in order to further develop intuition; these results are
presented in \red{SECTION X}.

