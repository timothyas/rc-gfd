\section{Forecasting with Recurrent Neural Networks}
\label{sec:rnn-architecture}

Our goal is to develop an emulator that can predict the future state of a
dynamical system, given an estimate of the initial state.
We assume the true dynamics of the system to be governed by the discrete-time
relation
\todo{this doesn't make sense. is sampled at discrete time? Once decided, finish
time notation unification}
\begin{linenomath*}\begin{equation}
    \state_{n+1} = f(\state_{n}) \, ,
\end{equation}\end{linenomath*}
where $\state_n \in\statespace$ is the system state at time $t_n = n\Delta t$,
$n\in\mathbb{Z}$, evolving according to $f(\cdot)$.
RNNs are a family of NNs which process information that as a sequence,
such as the chronological ordering time,
and we therefore consider them to be a natural choice for the forecasting
task.
Following \citet{goodfellow_sequence_2016}, we use the following general form
for RNNs in this work: \todo{Use input state here to allow for more general
    setting}
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \hidden(t+1) &= g_{h}\left(
            \hidden(t), \state(t); \hyperparameters
            \right) \\
        \hat{\state}(t+1) &= g_{o} \left( \hidden(t+1) \right) \, .
    \end{aligned}
    \label{eq:rnn}
\end{equation}\end{linenomath*}
\todo{should include distinction between training and prediction phases}
Here
$\hidden(t)\in\hiddenspace$ is the state of the hidden layer at time $t$,
$g_{h}(\cdot)$ steps the hidden state forward in time
given the hyperparameters $\hyperparameters$,
and
$g_{o}(\cdot)$ is the output, or ``readout'' operation,
which maps the hidden state to the target system state $\hat{\state}$.
This formulation shows how RNNs can be interpreted as dynamical systems of their
own, where the evolution of the internal state, $\hidden$, is subject to the
external system and hidden states at the previous time step,
$\state(t-1)$ and $\hidden(t-1)$, respectively.

The RNN architectures that we use in this work employ common
simplifications that are relevant to the readout operator and training
procedure; we discuss this in \cref{subsec:readout}.
Additionally, we employ a similar strategy to parallelize the architecture for
high dimensional systems, and this is discussed in
\cref{subsec:parallelization}.
Finally, the specific form of $g_h(\cdot)$ for the RC and NVAR architectures
is provided in \cref{subsec:rc,subsec:nvar},
respectively.


\subsection{Simplified Readout and Training}
\label{subsec:readout}

The RNN architectures that we use in this study employ two
simplifications relative to the generic form presented in
\cref{eq:rnn}.
First, any internal connections between the nodes of the hidden state,
$\hidden(t)$, are pre-defined by the model hyperparameters.
That is, no internal weights contained within $g_h(\cdot)$
are learned during the training process.
Secondly, the readout operator is linear, such that
\begin{linenomath*}\begin{equation*}
    g_o(\hidden(t)) \coloneqq \Wout \hidden(t) \, ,
\end{equation*}\end{linenomath*}
where $\Wout \in \Woutspace$ is a matrix.
The result of these two assumptions is a cost function that is quadratic with
respect to the elements of $\Wout$.

To be more precise, the goal of the training process is to find the matrix
elements which minimize the regularized, model-data misfit cost function:
\begin{linenomath*}\begin{equation}
    \cf(\Wout) =
        \dfrac{1}{2}\sum_{n=1}^{\ntrain} \norm{\Wout \hidden_n - \state_n}^2
        +
        \dfrac{\beta}{2}\norm{\Wout}_\text{F}^2 \, .
    \label{eq:cost}
\end{equation}\end{linenomath*}
Here
$\norm{\mathbf{A}}_\text{F} \coloneqq
\sqrt{\text{Tr}\left(\mathbf{A}\mathbf{A}^T\right)}$
is the Frobenius norm,
$\ntrain$ is the number of time steps used for training,
$\beta$ is a Tikhonov regularization parameter \citep{tikhonov_solution_1963}, chosen to improve
numerical stability and prevent overfitting.

The hidden and target states can be expressed in matrix form by concatenating
each time step ``column-wise'':
$\Hidden \coloneqq (\hidden_1 \, \hidden_2 \, \cdots \, \hidden_{\ntrain})$,
and similarly
$\State \coloneqq (\state_1 \, \state_2 \, \cdots \, \state_{\ntrain})$.
With this notation, the elements of $\Wout$ can be compactly written as the
solution to the linear ridge regression problem
\begin{equation}
    \Wout = \State \Hidden^T \left(\Hidden\Hidden^T + \beta\right)^{-1} \, ,
    \label{eq:ridge_regression}
\end{equation}
although we do not form the inverse explicitly.
We instead use the \texttt{solve} function from SciPy's linear algebra module
\citep{scipy_2020}, based on performance and efficiency testing by
\citet{platt_systematic_2022}. \todo{talk about python implementation?}


\subsection{Parallelization Strategy}
\label{subsec:parallelization}

The RNN architectures that we use inherit the gridded structure of the target
state being emulated, and require hidden states that are
$\mathcal{O}(10-100)$ larger.
Atmosphere and ocean GCMs typically propagate high dimensional state vectors,
$\mathcal{O}(>10^6)$,
so representing the system with a single hidden state would be intractable.
Thus, we employ a parallelization strategy to distribute the target and hidden
states across many semi-independent RNNs.
Our strategy follows the algorithm introduced by \citet{pathak_model-free_2018},
and follows the same general construction as \citet{arcomano_machine_2020}.
We outline the procedure here and note an illustration of the process in
\red{FIGURE}.

We subdivide the domain into $\ngroups$ groups based on horizontal location,
akin to typical domain decomposition for atmosphere and ocean
GCMs on structured grids \red{CITE?}.
Each group contains
$\nlocalstate\times\nlocalstate$ horizontal grid cells, and all $\nvertical$
vertical grid cells at each horizontal location.
The global state vector, $\state$, which consists of all state variables to be
emulated at all grid cells, is partitioned into $\ngroups$ local state vectors,
$\localstate$, where $i$ and $j$ denote the relative position of the group in
each horizontal direction.
In order to facilitate interactions between nearby groups, each group
has a designated overlap region which consists of the $\nneighbor$ elements
from its neighboring groups.
The local state vectors, plus elements from the overlap region, are concatenated
to form local input state vectors, $\localinputstate$.
These local input vectors drive separate RNNs at each group, thereby generating
distinct hidden states for each group as follows
\begin{linenomath*}\begin{equation}
    \begin{aligned}
        \localhidden^{n+1}
        &= g_h\left(
            \localhidden^n, \localinputstate^n; \hyperparameters
        \right) \\
        \localoutput^{n+1}
        &= g_o\left(\localhidden^{n+1}\right) \, .
    \end{aligned}
    \label{eq:local-rnn}
\end{equation}\end{linenomath*}
We make the assumption that the hyperparameters which determine internal
connections within $g_h(\cdot)$ are globally fixed, and therefore the only
aspect which drives unique hidden states in each groups is the input vector
$\localinputstate$.
For concreteness, we re-state this assumption as it pertains to each
architecture in \cref{subsec:rc,subsec:nvar}.

During the training phase, each group acts completely independent from one
another.
Therefore, the training process is embarrassingly parallel and allows us to
scale the problem to arbitrarily large state vectors across a distributed
computing system, subject to resource constraints.
During the prediction phase, neighboring elements must be passed between
groups in order to fill each overlap region at each time step.
We note that this requirement is not overbearing, as GCMs require similar
message passing between neighboring processes in order to compute spatial
derivatives and perform interpolation.



\subsection{Reservoir Computing}
\label{subsec:rc}

Motivation scratch:
\begin{itemize}
    \item Interested in RC and NVAR, which have a very similar interpretation
    \item RC has been shown to have good prediction skill in forecasting chaotic
        system dynamics, making it an attractive architecture for weather and
        climate forecasting
    \item Shown to accurately reproduce lyapunov spectrum of chaotic systems,
        capturing positive lyapunov exponents, an important aspect of error
        growth relevant for data assimilation
    \item \citep{penny_integrating_2022} showed explicitly that RC can be
        integrated into DA algorithms used by modern forecasting centers
    \item Moreover,
        RC was originally developed as a way to bypass the problem of slow
        convergence in training, hindering its use in large scale applications.
        More recently,
        RC has been shown to have similar prediction skill as more
        complicated
        RNNs, like LSTMs \citep{vlachas_backpropagation_2020}
        It therefore appears the random instantiation of RC affords some
        robustness in addition to speed... see \citet{bollt_explaining_2021} for
        intuition on how RC ``does well''.
\end{itemize}

Reservoir computing in general:
\begin{itemize}
    \item equations
    \item hyperparameters
    \item some interpretation of what's going on
    \item note when talking about linear:
        Additionally, the readout function is taken to be linear, such that
        $f(x) = Wx$ where W is a matrix. We make this choice because
        this was shown to perform equally well when compared to more complex,
        nonlinear readout operations \citep{platt_systematic_2022}.
        We clarify, however, that both architectures contain nonlinearity in
        the operation $g_h(\cdot)$, and this nonlinearity is essential for
        accurate emulation \red{See Fig} \citep{bollt_explaining_2021}.
\end{itemize}

\subsection{Nonlinear Vector Auto-Regression}
\label{subsec:nvar}

NVAR
\begin{itemize}
    \item We are motivated to employ the NVAR architecture based on the work by
        \citep{gauthier_next_2021} (NG/NVAR disambiguation...)
    \item In essence, for relatively simple problems, NVAR is shown to have
        similar or better prediction skill as RC.
\end{itemize}

Simplification of RC.

